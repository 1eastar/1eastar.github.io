---
permalink: /
title: "Dongjin Kang"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<p style="font-size:14px;">
Hi! I am a M.S. student at <a href="https://langlab.yonsei.ac.kr" target="_blank">LangAGI Lab</a> advised by <a href="https://jinyeo.weebly.com/" target="_blank">Jinyoung Yeo</a>. Previously, I received B.S. in Computer Science from Yonsei University in Feb. 2024.
<br/><br/>
My recent research interests include: (i) Reasoning and Planning to solve long-horizon tasks and (ii) Embodied AI with a strong understanding of real-world dynamics. Additionally, I focus on analyzing language models (LMs) to identify limitations and room for improvement. The ultimate goal of my research is to design systems that enable humans to communicate and interact with AI in a trustworthy and beneficial manner.
<br/><br/>
Topics of interest
<br/><br/>
Reasoning and Planning: Think-and-Execute, RewardMATH<br/>
Embodied AI:<br/>
Analysis of LMs: Preference Bias, Cactus, RewardMATH<br/>
</p>


## Publications

<p style="font-size:15px;">
<sup>‡</sup> indicates equal contribution.
</p>

### Under-review/Preprints
<p style="font-size:18px; margin-bottom: 0.3em;">
Web-Shepherd: Advancing PRMs for Reinforcing Web Agents
</p>

<p style="font-size:15px;">
Hyungjoo Chae<sup>‡</sup>, Sunghwan Kim<sup>‡</sup>, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, Dongwook Choi, Minseok Kang, Gwanhoon Im, ByeongUng Cho, Hyojun Kim, Jun Hee Han, Taeyoon Kwon, Minju Kim, Beong-woo Kwak, <u style="font-weight:bold;">Dongjin Kang</u>, Jinyoung Yeo
<br/>

<br/>
<a href="https://arxiv.org/abs/2505.15277" target="_blank">[paper]</a>   <a href="https://github.com/kyle8581/Web-Shepherd" target="_blank">[code]</a>
</p>
___
<p style="font-size:18px; margin-bottom: 0.3em;">
ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions
</p>

<p style="font-size:15px;">
Beong-woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, <u style="font-weight:bold;">Dongjin Kang</u>, Sunghwan Kim, Dongil Yang, Jinyoung Yeo
<br/>

<br/>
<a href="https://1eastar.github.io" target="_blank">[paper]</a>
</p>

___


### 2025
<p style="font-size:18px; margin-bottom: 0.3em;">
Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization
</p>

<p style="font-size:15px;">
Sunghwan Kim<sup>‡</sup>, <u style="font-weight:bold;">Dongjin Kang</u><sup>‡</sup>, Taeyoon Kwon, Hyungjoo Chae, Dongha Lee, Jinyoung Yeo
<br/><br/>
ACL 2025 main
<br/>
<a href="https://arxiv.org/abs/2505.12763" target="_blank">[paper]</a>   <a href="https://github.com/kimsh0507/rethinking_rm_eval" target="_blank">[code]</a>
</p>
___
<p style="font-size:18px; margin-bottom: 0.3em;">
One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL
</p>

<p style="font-size:15px;">
Hyungjoo Chae<sup>‡</sup>, <u style="font-weight:bold;">Dongjin Kang</u><sup>‡</sup>, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee
<br/><br/>
ACL 2024 Industry
<br/>
<a href="https://1eastar.github.io" target="_blank">[paper]</a>   <a href="https://github.com/LGAI-Research/Long-CoT-Collection" target="_blank">[code]</a>
</p>

___


### 2024
<p style="font-size:18px; margin-bottom: 0.3em;">
Evaluating Robustness of Reward Models for Mathematical Reasoning
</p>

<p style="font-size:15px;">
Sunghwan Kim<sup>‡</sup>, <u style="font-weight:bold;">Dongjin Kang</u><sup>‡</sup>, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, Jinyoung Yeo
<br/><br/>
Arxiv preprint.
<br/>
<a href="https://arxiv.org/abs/2410.01729" target="_blank">[paper]</a>   <a href="https://github.com/kimsh0507/RewardMATH_official" target="_blank">[code]</a>
</p>
___
<p style="font-size:18px; margin-bottom: 0.3em;">
Coffee-gym: An environment for evaluating and improving natural language feedback on erroneous code
</p>

<p style="font-size:15px;">
Hyungjoo Chae<sup>‡</sup>, Taeyoon Kwon<sup>‡</sup>, Seungjun Moon<sup>‡</sup>, Yongho Song, <u style="font-weight:bold;">Dongjin Kang</u>, Kai Tzu-iunn Ong, Beong-woo Kwak, Seonghyeon Bae, Seung-won Hwang, Jinyoung Yeo
<br/><br/>
EMNLP 2024 main
<br/>
<a href="https://arxiv.org/abs/2409.19715" target="_blank">[paper]</a>  <a href="https://huggingface.co/spaces/Coffee-Gym/Project-Coffee-Gym" target="_blank">[demo]</a>
</p>

___
<p style="font-size:18px; margin-bottom: 0.3em;">
Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory
</p>

<p style="font-size:15px;">
Suyeon Lee<sup>‡</sup>, Sunghwan Kim<sup>‡</sup>, Minju Kim<sup>‡</sup>, <u style="font-weight:bold;">Dongjin Kang</u>, Dongil Yang, Harim Kim, Minseok Kang, Dayi Jung, Min Hee Kim, Seungbeen Lee, Kyoung-Mee Chung, Youngjae Yu, Dongha Lee, Jinyoung Yeo

<br/><br/>
EMNLP 2024 findings
<br/>
<a href="https://arxiv.org/abs/2407.03103" target="_blank">[paper]</a>
</p>

___
<p style="font-size:18px; margin-bottom: 0.3em;">
Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation
</p>

<p style="font-size:15px;">
<u style="font-weight:bold;">Dongjin Kang</u><sup>‡</sup>, Sunghwan Kim<sup>‡</sup>, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo

<br/><br/>
<u style="font-weight:900; color: #ea3800">[Outstanding Paper Award]</u> ACL 2024 main
<br/>
<a href="https://arxiv.org/abs/2402.13211" target="_blank">[paper]</a>    <a href="https://github.com/1eastar/emotionalsupport" target="_blank">[code]</a>
</p>

___
<p style="font-size:18px; margin-bottom: 0.3em;">
Coffee: Boost your code llms by fixing bugs with feedback
</p>

<p style="font-size:15px;">
Seungjun Moon<sup>‡</sup>, Yongho Song<sup>‡</sup>, Hyungjoo Chae<sup>‡</sup>, Taeyoon Kwon, <u style="font-weight:bold;">Dongjin Kang</u>, Kai Tzu-iunn Ong, Seung-won Hwang, Jinyoung Yeo
<br/><br/>
Arxiv Preprint
<br/>
<a href="https://arxiv.org/abs/2311.07215" target="_blank">[paper]</a>    <a href="https://kyle8581.github.io/COFFEE-DEMO" target="_blank">[demo]</a>
</p>

___
<p style="font-size:18px; margin-bottom: 0.3em;">
Large language models are clinical reasoners: Reasoning-aware diagnosis framework with prompt-generated rationales
</p>

<p style="font-size:15px;">
Taeyoon Kwon<sup>‡</sup>, Kai Tzu-iunn Ong<sup>‡</sup>, <u style="font-weight:bold;">Dongjin Kang</u>, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo
<br/><br/>
AAAI 2024
<br/>
<a href="https://arxiv.org/abs/2312.07399" target="_blank">[paper]</a>
</p>

